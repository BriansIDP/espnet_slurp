# network architecture
backend: pytorch
model-module: espnet.nets.pytorch_backend.e2e_asr_cfmlas:E2E
# encoder related
elayers: 16
eunits: 2048
eprojs: 512 # this must equal to transformer-adim
transformer-adim: 512
transformer-aheads: 4
transformer-attn-dropout-rate: 0.0
transformer-input-layer: conv2d
transformer-encoder-activation-type: swish
transformer-encoder-pos-enc-layer-type: rel_pos
transformer-encoder-selfattn-layer-type: rel_selfattn
macaron-style: true
use-cnn-module: true
cnn-module-kernel: 31
# decoder related
dlayers: 1
dunits: 1024
# attention related
adim: 1024
atype: location
aconv-chans: 10
aconv-filts: 100

# hybrid CTC/attention
mtlalpha: 0.0

# label smoothing
lsm-type: unigram
lsm-weight: 0.1
dropout-rate: 0.1
dropout-rate-decoder: 0.1
weight-decay: 0.0
# ema-decay: 0.999
context-residual: true

# minibatch related
batch-size: 20
maxlen-in: 512
maxlen-out: 150
maxioratio: 300
minioratio: 6

# optimization related
sortagrad: 0
accum-grad: 6
grad-clip: 5
opt: noam
epochs: 30
patience: 0
# weight-noise-std: 0.00
# weight-noise-start: 20000
transformer-lr: 0.1
transformer-warmup-steps: 600

# scheduled sampling option
sampling-probability: 0.0
report-interval-iters: 20
# freeze-mod: enc,att

# KB related
### initialise from the checkpoint of the pretrained ASR model (trained 20 epochs on Librispeech 960-hour)
# init-full-model: /home/gs534/rds/hpc-work/work/Librispeech/exp/librispeech_train_conformer_KB1000_KB256_spec_drop_gcn2l/results/model.acc.best
acousticonly: true
attn_dim: 256
KBmaxlen: 200
KBminlen: 200
randomKBsample: true
DBdrop: 0.3
treetype: gcn2

# Modality matching
modalitymatch: true
mmfactor: 0.0
pooling: index

# SLU
slotfile: data/SLU/slottypes.txt
intentfile: data/SLU/intents.txt # although intent classification is not a task in this case
doslu: true
slotfactor: 1.0
intentfactor: 0.0
jointrep: true
robertamask: 0.5
### Option: initialise from a specific model checkpoint; by default it is huggingface pretrained model
# init-preLM: /home/gs534/rds/hpc-work/work/transformers/examples/pytorch/language-modeling/gpt2_output/SLURP_gpt2_wordLM0.0_intent0.0_slot1.0_pairs_full_parallel/gpt_state_dict
jointptrgen: true

slotKB: true
topnslot: 3
### Toggle between wordlevel and entitylevel KB
ontology: data/KB/word_ontology_f30.json
# ontology: data/KB/ontologymerge_entf30_unigram600suffix.json

